{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a101d12-2f24-4e2c-8a4d-2127fa6b2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFICATION ---\n",
    "# Added 'ast' to safely parse string representations of lists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import ast\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7083eb5b-eaf9-4286-88f5-50cc5d7f3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "# --- MODIFICATION ---\n",
    "# The checkpoint directory can remain the same as we are using the same sentence-level model\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"results\", f\"regression_{MODEL_NAME.split('/')[-1]}\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "# These are the original SENTENCE-LEVEL files for training the model\n",
    "BAREC_TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "BAREC_DEV_PATH = os.path.join(DATA_DIR, 'dev.csv')\n",
    "\n",
    "# --- MODIFICATION ---\n",
    "# New path for the DOCUMENT-LEVEL blind test file\n",
    "# Assuming your document test file is named 'doc_blind_test_data.csv'\n",
    "DOC_BLIND_TEST_PATH = os.path.join(DATA_DIR, 'doc_blind_test.csv') \n",
    "\n",
    "# --- MODIFICATION ---\n",
    "# Updated submission file names for clarity\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.zip\")\n",
    "\n",
    "# Preprocessed sentence file paths (these do not change as training data is the same)\n",
    "TRAIN_PREPROCESSED_PATH = os.path.join(DATA_DIR, 'train_preprocessedv2.csv')\n",
    "DEV_PREPROCESSED_PATH = os.path.join(DATA_DIR, 'dev_preprocessedv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd0110-a75b-420b-9736-e0b6119f3245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1834d5-158c-480d-9fcd-bf712326605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT Disambiguator for preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading BAREC SENTENCE-LEVEL Data for Training ---\n",
      "âœ” Found preprocessed sentence files. Loading them directly...\n",
      "Successfully loaded 54845 training and 7310 validation records.\n",
      "\n",
      "===== INITIALIZING REGRESSION MODEL AND TRAINER =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\barec_env\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentence-level model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1122' max='20568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1122/20568 03:54 < 1:07:44, 4.78 it/s, Epoch 0.33/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 150\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Check if a trained model already exists to avoid re-training\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CHECKPOINT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ” Training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_model(CHECKPOINT_DIR)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\barec_env\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\barec_env\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- DATA LOADING AND PREPROCESSING ---\n",
    "# This function remains UNCHANGED as it works on sentence strings\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    \"\"\"\n",
    "    Preprocesses text into the D3Tok format using BERTUnfactoredDisambiguator.\n",
    "    This version includes robust error handling for missing 'd3tok' keys.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else:\n",
    "                d3tok_forms.append(disambig_word.word)\n",
    "        else:\n",
    "            d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n",
    "\n",
    "\n",
    "# This function remains UNCHANGED as it prepares the SENTENCE-LEVEL training data\n",
    "def load_or_preprocess_data(disambiguator):\n",
    "    \"\"\"\n",
    "    Loads preprocessed data if it exists, otherwise, it runs preprocessing.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading BAREC SENTENCE-LEVEL Data for Training ---\")\n",
    "    if os.path.exists(TRAIN_PREPROCESSED_PATH) and os.path.exists(DEV_PREPROCESSED_PATH):\n",
    "        print(\"âœ” Found preprocessed sentence files. Loading them directly...\")\n",
    "        train_df = pd.read_csv(TRAIN_PREPROCESSED_PATH)\n",
    "        val_df = pd.read_csv(DEV_PREPROCESSED_PATH)\n",
    "        train_df['text'] = train_df['text'].astype(str)\n",
    "        val_df['text'] = val_df['text'].astype(str)\n",
    "        print(f\"Successfully loaded {len(train_df)} training and {len(val_df)} validation records.\")\n",
    "        return train_df, val_df\n",
    "    else:\n",
    "        print(\"Preprocessed files not found. Starting one-time preprocessing on sentence data...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv(BAREC_TRAIN_PATH)\n",
    "            val_df = pd.read_csv(BAREC_DEV_PATH)\n",
    "            train_df = train_df[['Sentence', 'Readability_Level_19']].rename(\n",
    "                columns={'Sentence': 'text', 'Readability_Level_19': 'label'})\n",
    "            val_df = val_df[['Sentence', 'Readability_Level_19']].rename(\n",
    "                columns={'Sentence': 'text', 'Readability_Level_19': 'label'})\n",
    "            train_df.dropna(subset=['text', 'label'], inplace=True)\n",
    "            val_df.dropna(subset=['label', 'text'], inplace=True)\n",
    "            train_df['text'] = train_df['text'].astype(str)\n",
    "            val_df['text'] = val_df['text'].astype(str)\n",
    "            train_df['label'] = train_df['label'].astype(int) - 1\n",
    "            val_df['label'] = val_df['label'].astype(int) - 1\n",
    "            train_df['label'] = train_df['label'].astype(float)\n",
    "            val_df['label'] = val_df['label'].astype(float)\n",
    "            print(f\"Successfully loaded raw sentence data: {len(train_df)} training and {len(val_df)} validation records.\")\n",
    "            print(\"\\n--- Preprocessing Text to D3Tok format (this will only run once) ---\")\n",
    "            train_df['text'] = train_df['text'].apply(lambda x: preprocess_d3tok(x, disambiguator))\n",
    "            val_df['text'] = val_df['text'].apply(lambda x: preprocess_d3tok(x, disambiguator))\n",
    "            print(\"âœ” Text preprocessing finished.\")\n",
    "            print(\"\\n--- Saving preprocessed data for future use... ---\")\n",
    "            train_df.to_csv(TRAIN_PREPROCESSED_PATH, index=False)\n",
    "            val_df.to_csv(DEV_PREPROCESSED_PATH, index=False)\n",
    "            print(f\"** Saved preprocessed files to {TRAIN_PREPROCESSED_PATH} and {DEV_PREPROCESSED_PATH} **\")\n",
    "            return train_df, val_df\n",
    "        except FileNotFoundError:\n",
    "            print(f\"! ERROR: Raw file not found. Make sure sentence-level 'train.csv' and 'dev.csv' are in the '{DATA_DIR}' directory.\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"! ERROR during initial processing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "print(\"Initializing BERT Disambiguator for preprocessing...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "\n",
    "train_df, val_df = load_or_preprocess_data(bert_disambiguator)\n",
    "\n",
    "if train_df is not None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "else:\n",
    "    print(\"Stopping script due to data loading failure.\")\n",
    "    exit()\n",
    "\n",
    "# --- DATASET AND METRICS ---\n",
    "# This class remains UNCHANGED\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "# This function remains UNCHANGED\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.flatten()\n",
    "    rounded_preds = np.round(preds)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1).astype(int)\n",
    "    labels = p.label_ids.astype(int)\n",
    "    qwk = cohen_kappa_score(labels, clipped_preds, weights='quadratic')\n",
    "    return {\"qwk\": qwk}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ed3c3-d1a8-4789-8f35-72cd6f8b1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MODEL TRAINING ---\n",
    "# This entire block remains UNCHANGED. We are training the sentence-level model.\n",
    "print(\"\\n===== INITIALIZING REGRESSION MODEL AND TRAINER =====\\n\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "train_dataset = ReadabilityDataset(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "val_dataset = ReadabilityDataset(val_df['text'].tolist(), val_df['label'].tolist())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting sentence-level model training...\")\n",
    "# Check if a trained model already exists to avoid re-training\n",
    "if not os.path.exists(os.path.join(CHECKPOINT_DIR, \"pytorch_model.bin\")):\n",
    "    trainer.train()\n",
    "    print(\"âœ” Training finished.\")\n",
    "    trainer.save_model(CHECKPOINT_DIR)\n",
    "    print(f\"Model saved to {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    print(f\"âœ” Found existing trained model in {CHECKPOINT_DIR}. Skipping training.\")\n",
    "    # We still need to load the best model into the trainer for prediction\n",
    "    # The `Trainer` class loads the best checkpoint automatically if `load_best_model_at_end=True`\n",
    "    # and training was completed in a previous run. If not, we re-instantiate it.\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)\n",
    "    trainer = Trainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2666d-8bff-4a2e-a9e6-96c413d4c380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694360b7-da8d-4e38-8579-181538190485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377a661d-af4e-44fa-9259-ef377da05a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFICATION: DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION ---\n",
    "# This entire block is rewritten to handle documents.\n",
    "print(\"\\n===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    # 1. Load the DOCUMENT test file\n",
    "    print(f\"Loading document test data from {DOC_BLIND_TEST_PATH}...\")\n",
    "    doc_test_df = pd.read_csv(DOC_BLIND_TEST_PATH)\n",
    "    doc_test_df.dropna(subset=['ID', 'Sentences'], inplace=True)\n",
    "    \n",
    "    # 2. Explode documents into a long list of sentences\n",
    "    # We create a new DataFrame where each row is a single sentence,\n",
    "    # but we keep track of which document it came from.\n",
    "    print(\"Processing documents: exploding into individual sentences...\")\n",
    "    all_sentences = []\n",
    "    doc_ids = []\n",
    "    for _, row in doc_test_df.iterrows():\n",
    "        doc_id = row['ID']\n",
    "        # The 'Sentences' column is a string representation of a list, e.g., \"['sent1', 'sent2']\"\n",
    "        # We use ast.literal_eval to safely parse it into a Python list.\n",
    "        try:\n",
    "            sentences_list = ast.literal_eval(row['Sentences'])\n",
    "            if sentences_list: # Only add if the list is not empty\n",
    "                all_sentences.extend(sentences_list)\n",
    "                doc_ids.extend([doc_id] * len(sentences_list))\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Warning: Could not parse sentences for document ID {doc_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "    sentence_df = pd.DataFrame({\n",
    "        'doc_id': doc_ids,\n",
    "        'sentence_text': all_sentences\n",
    "    })\n",
    "    print(f\"Successfully created {len(sentence_df)} sentences from {len(doc_test_df)} documents.\")\n",
    "\n",
    "    # 3. Preprocess all sentences at once\n",
    "    print(\"\\nPreprocessing all sentences to D3Tok format...\")\n",
    "    sentence_df['processed_text'] = sentence_df['sentence_text'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "    \n",
    "    # 4. Get predictions for ALL sentences in a single batch\n",
    "    print(\"Generating predictions for all sentences...\")\n",
    "    test_dataset = ReadabilityDataset(sentence_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    sentence_df['raw_prediction'] = predictions.predictions.flatten()\n",
    "    \n",
    "    # 5. Aggregate results: find the MAX prediction for each document\n",
    "    # This is the key step based on the hint.\n",
    "    print(\"Aggregating results: finding the max readability score per document...\")\n",
    "    doc_predictions = sentence_df.groupby('doc_id')['raw_prediction'].max()\n",
    "    \n",
    "    # 6. Post-process the final document predictions\n",
    "    rounded_preds = np.round(doc_predictions.values)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "    \n",
    "    # 7. Create the final submission file\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Sentence ID': doc_predictions.index, # The column name is 'Sentence ID' in the competition\n",
    "        'Prediction': (clipped_preds + 1).astype(int)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nSaving prediction file to: {SUBMISSION_PATH}\")\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"Compressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "        \n",
    "    print(f\"âœ” Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'doc_blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final document prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc9f80c-767b-4526-ba3f-c8086f4dbf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d88867f-5f2e-4345-aee9-568dd808f244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79cd8b-7777-4136-a297-9832835dc147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2494918-c16d-4b19-bf0d-f14007ac3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT Disambiguator for preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\n",
      "\n",
      "âœ” Found checkpoint at: D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\n",
      "Loading model from checkpoint...\n",
      "âœ” Model loaded successfully.\n",
      "\n",
      "===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Loading document test data from .\\data\\doc_blind_test_data.csv...\n",
      "Processing documents: breaking them down into individual sentences...\n",
      "Warning: Could not parse sentences for document ID 1010295. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010296. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010297. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010298. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010299. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010300. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010301. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010302. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010303. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010304. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010305. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010306. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010307. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010308. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010309. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010310. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010311. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010312. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010313. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010314. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010315. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010316. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010317. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1010318. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1020024. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1020025. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1020026. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1020027. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1030016. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1030025. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1030048. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1040002. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1040014. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1040046. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1050019. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1050035. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1050052. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1060011. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1060026. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1060047. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 1120001. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010025. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010053. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010076. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010093. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010110. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010127. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2010141. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020013. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020014. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020015. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020016. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020102. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020103. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020104. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020105. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020106. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2020107. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2030023. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2030030. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2030044. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2030045. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2040007. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2040027. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2040046. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2040056. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150001. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150002. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150003. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150004. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150005. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150006. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150007. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150008. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150009. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150010. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150011. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150012. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150013. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150014. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150015. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150016. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150017. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150018. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 2150019. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3010063. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3010071. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3010097. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3010103. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3010112. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3010116. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3030032. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3030034. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3030043. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3030051. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3030052. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3030062. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3040010. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3050013. Skipping this document.\n",
      "Warning: Could not parse sentences for document ID 3050029. Skipping this document.\n",
      "Successfully created 0 sentences from 100 documents.\n",
      "\n",
      "Preprocessing all sentences to D3Tok format (this may take a moment)...\n",
      "Generating predictions for all sentences...\n",
      "An error occurred during final document prediction: list index out of range\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import ast # To parse string representations of lists\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "# Document test file\n",
    "DOC_BLIND_TEST_PATH = os.path.join(DATA_DIR, 'doc_blind_test_data.csv') \n",
    "# Submission files\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.zip\")\n",
    "\n",
    "\n",
    "# --- DATA PREPROCESSING (UNCHANGED) ---\n",
    "# This function preprocesses a single sentence.\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    \"\"\"\n",
    "    Preprocesses text into the D3Tok format using BERTUnfactoredDisambiguator.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else:\n",
    "                d3tok_forms.append(disambig_word.word)\n",
    "        else:\n",
    "            d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n",
    "\n",
    "print(\"Initializing BERT Disambiguator for preprocessing...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# --- DATASET CLASS (UNCHANGED) ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "\n",
    "# --- MODEL LOADING ---\n",
    "# --- THIS IS THE KEY MODIFIED SECTION ---\n",
    "print(\"\\n===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\\n\")\n",
    "\n",
    "# --- CHANGE 1: Set this variable to your exact checkpoint path.\n",
    "# Using r\"...\" (raw string) is the best practice for Windows paths.\n",
    "CHECKPOINT_DIR = r\"D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\"\n",
    "\n",
    "# --- CHANGE 2: Check for \"model.safetensors\" in the specified directory.\n",
    "if os.path.exists(os.path.join(CHECKPOINT_DIR, \"model.safetensors\")):\n",
    "    print(f\"âœ” Found checkpoint at: {CHECKPOINT_DIR}\")\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    \n",
    "    # Load the already trained model from the specified checkpoint directory\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)\n",
    "    \n",
    "    # We only need a minimal trainer object for the .predict() method\n",
    "    trainer = Trainer(model=model)\n",
    "    print(\"âœ” Model loaded successfully.\")\n",
    "\n",
    "else:\n",
    "    # If the checkpoint isn't found, print a clear error message and exit.\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"! ERROR: Checkpoint not found at the specified path.\")\n",
    "    print(f\"! Searched for 'model.safetensors' inside: {CHECKPOINT_DIR}\")\n",
    "    print(\"! Please ensure the CHECKPOINT_DIR variable in the script is correct.\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    exit() # Stop the script\n",
    "\n",
    "\n",
    "# --- DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION ---\n",
    "print(\"\\n===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    # 1. Load the DOCUMENT test file\n",
    "    print(f\"Loading document test data from {DOC_BLIND_TEST_PATH}...\")\n",
    "    doc_test_df = pd.read_csv(DOC_BLIND_TEST_PATH)\n",
    "    doc_test_df.dropna(subset=['ID', 'Sentences'], inplace=True)\n",
    "    \n",
    "    # 2. Explode documents into a long list of sentences\n",
    "    print(\"Processing documents: breaking them down into individual sentences...\")\n",
    "    all_sentences = []\n",
    "    doc_ids = []\n",
    "    for _, row in doc_test_df.iterrows():\n",
    "        doc_id = row['ID']\n",
    "        # The 'Sentences' column is a string like \"['sent1', 'sent2']\"\n",
    "        # ast.literal_eval safely converts this string into a Python list\n",
    "        try:\n",
    "            sentences_list = ast.literal_eval(row['Sentences'])\n",
    "            if sentences_list: # Only process if the list is not empty\n",
    "                all_sentences.extend(sentences_list)\n",
    "                doc_ids.extend([doc_id] * len(sentences_list))\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Warning: Could not parse sentences for document ID {doc_id}. Skipping this document.\")\n",
    "            continue\n",
    "            \n",
    "    sentence_df = pd.DataFrame({\n",
    "        'doc_id': doc_ids,\n",
    "        'sentence_text': all_sentences\n",
    "    })\n",
    "    print(f\"Successfully created {len(sentence_df):,} sentences from {len(doc_test_df):,} documents.\")\n",
    "\n",
    "    # 3. Preprocess all sentences\n",
    "    print(\"\\nPreprocessing all sentences to D3Tok format (this may take a moment)...\")\n",
    "    sentence_df['processed_text'] = sentence_df['sentence_text'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "    \n",
    "    # 4. Get predictions for ALL sentences using the loaded model\n",
    "    print(\"Generating predictions for all sentences...\")\n",
    "    test_dataset = ReadabilityDataset(sentence_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    sentence_df['raw_prediction'] = predictions.predictions.flatten()\n",
    "    \n",
    "    # 5. Aggregate results: find the MAX prediction for each document\n",
    "    print(\"Aggregating results: finding the max readability score per document...\")\n",
    "    doc_predictions = sentence_df.groupby('doc_id')['raw_prediction'].max()\n",
    "    \n",
    "    # 6. Post-process the final document predictions\n",
    "    rounded_preds = np.round(doc_predictions.values)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "    \n",
    "    # 7. Create the final submission file\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Sentence ID': doc_predictions.index, # The column name is 'Sentence ID' in the competition\n",
    "        'Prediction': (clipped_preds + 1).astype(int)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nSaving prediction file to: {SUBMISSION_PATH}\")\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"Compressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "        \n",
    "    print(f\"âœ” Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'doc_blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final document prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf0e06b-52e1-4bf2-aa1b-a9d797ef26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CAMeL Tools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Tools initialized.\n",
      "\n",
      "===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\n",
      "\n",
      "âœ” Found checkpoint at: D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\n",
      "Loading model from checkpoint...\n",
      "âœ” Model loaded successfully.\n",
      "\n",
      "===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Loading document test data from .\\data\\doc_blind_test_data.csv...\n",
      "Processing documents: tokenizing into sentences using pyarabic...\n",
      "Successfully created 100 sentences from 100 documents.\n",
      "\n",
      "Preprocessing all sentences to D3Tok format (this may take a moment)...\n",
      "An error occurred during final document prediction: 'LFUCache' object has no attribute '_LFUCache__links'\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "# --- MODIFICATION: Use pyarabic for sentence splitting ---\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "# --- These camel-tools imports are correct and unchanged ---\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "DOC_BLIND_TEST_PATH = os.path.join(DATA_DIR, 'doc_blind_test_data.csv') \n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.zip\")\n",
    "\n",
    "\n",
    "# --- DATA PREPROCESSING (UNCHANGED) ---\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else: d3tok_forms.append(disambig_word.word)\n",
    "        else: d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n",
    "\n",
    "# --- Initialize Tools (camel-tools only, pyarabic is used directly) ---\n",
    "print(\"Initializing CAMeL Tools...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"âœ” Tools initialized.\")\n",
    "\n",
    "\n",
    "# --- DATASET CLASS (UNCHANGED) ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "\n",
    "# --- MODEL LOADING (UNCHANGED) ---\n",
    "print(\"\\n===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\\n\")\n",
    "CHECKPOINT_DIR = r\"D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\"\n",
    "if os.path.exists(os.path.join(CHECKPOINT_DIR, \"model.safetensors\")):\n",
    "    print(f\"âœ” Found checkpoint at: {CHECKPOINT_DIR}\")\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)\n",
    "    trainer = Trainer(model=model)\n",
    "    print(\"âœ” Model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"! ERROR: Checkpoint not found at '{CHECKPOINT_DIR}'. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION (FIXED) ---\n",
    "print(\"\\n===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    print(f\"Loading document test data from {DOC_BLIND_TEST_PATH}...\")\n",
    "    doc_test_df = pd.read_csv(DOC_BLIND_TEST_PATH)\n",
    "    doc_test_df.dropna(subset=['ID', 'Document'], inplace=True)\n",
    "    \n",
    "    print(\"Processing documents: tokenizing into sentences using pyarabic...\")\n",
    "    all_sentences, doc_ids = [], []\n",
    "    for _, row in doc_test_df.iterrows():\n",
    "        doc_id = row['ID']\n",
    "        full_document_text = row['Document']\n",
    "        \n",
    "        if isinstance(full_document_text, str) and full_document_text.strip():\n",
    "            # --- FIX: Use pyarabic.araby.sentence_tokenize ---\n",
    "            sentences_list = araby.sentence_tokenize(full_document_text)\n",
    "            \n",
    "            if sentences_list:\n",
    "                all_sentences.extend(sentences_list)\n",
    "                doc_ids.extend([doc_id] * len(sentences_list))\n",
    "        else:\n",
    "            print(f\"Warning: Document ID {doc_id} has empty or invalid text. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not all_sentences:\n",
    "        print(\"\\n! ERROR: No sentences were extracted. Check the 'Document' column in your CSV.\")\n",
    "        exit()\n",
    "\n",
    "    sentence_df = pd.DataFrame({'doc_id': doc_ids, 'sentence_text': all_sentences})\n",
    "    print(f\"Successfully created {len(sentence_df):,} sentences from {len(doc_test_df):,} documents.\")\n",
    "\n",
    "    print(\"\\nPreprocessing all sentences to D3Tok format (this may take a moment)...\")\n",
    "    sentence_df['processed_text'] = sentence_df['sentence_text'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "    \n",
    "    print(\"Generating predictions for all sentences...\")\n",
    "    test_dataset = ReadabilityDataset(sentence_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    sentence_df['raw_prediction'] = predictions.predictions.flatten()\n",
    "    \n",
    "    print(\"Aggregating results: finding the max readability score per document...\")\n",
    "    doc_predictions = sentence_df.groupby('doc_id')['raw_prediction'].max()\n",
    "    \n",
    "    rounded_preds = np.round(doc_predictions.values)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({'Sentence ID': doc_test_df['ID']})\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Sentence ID': doc_predictions.index, \n",
    "        'Prediction': (clipped_preds + 1).astype(int)\n",
    "    })\n",
    "    final_submission_df = final_submission_df.merge(pred_df, on='Sentence ID', how='left')\n",
    "    final_submission_df['Prediction'].fillna(1, inplace=True)\n",
    "    final_submission_df['Prediction'] = final_submission_df['Prediction'].astype(int)\n",
    "\n",
    "    print(f\"\\nSaving prediction file to: {SUBMISSION_PATH}\")\n",
    "    final_submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"Compressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "        \n",
    "    print(f\"âœ” Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'doc_blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final document prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25dac04-5590-449e-a09f-558a3b7a160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CAMeL Tools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Tools initialized.\n",
      "\n",
      "===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\n",
      "\n",
      "âœ” Found checkpoint at: D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\n",
      "Loading model from checkpoint...\n",
      "âœ” Model loaded successfully.\n",
      "\n",
      "===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Loading document test data from .\\data\\doc_blind_test_data.csv...\n",
      "Processing documents: tokenizing into sentences using pyarabic...\n",
      "Successfully created 100 sentences from 100 documents.\n",
      "\n",
      "Preprocessing all sentences to D3Tok format (this may take a moment)...\n",
      "Generating predictions for all sentences...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating results: finding the max readability score per document...\n",
      "\n",
      "Saving prediction file to: .\\submission\\submission_document_regression_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatima\\AppData\\Local\\Temp\\ipykernel_14292\\150345311.py:139: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_submission_df['Prediction'].fillna(1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing submission_document_regression_final.csv into submission_document_regression_final.zip...\n",
      "âœ” Submission file submission_document_regression_final.zip created successfully.\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "# --- MODIFICATION: Use pyarabic for sentence splitting ---\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "# --- These camel-tools imports are correct and unchanged ---\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "DOC_BLIND_TEST_PATH = os.path.join(DATA_DIR, 'doc_blind_test_data.csv') \n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.zip\")\n",
    "\n",
    "\n",
    "# --- DATA PREPROCESSING (UNCHANGED) ---\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else: d3tok_forms.append(disambig_word.word)\n",
    "        else: d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n",
    "\n",
    "# --- Initialize Tools (camel-tools only, pyarabic is used directly) ---\n",
    "print(\"Initializing CAMeL Tools...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"âœ” Tools initialized.\")\n",
    "\n",
    "\n",
    "# --- DATASET CLASS (UNCHANGED) ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "\n",
    "# --- MODEL LOADING (UNCHANGED) ---\n",
    "print(\"\\n===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\\n\")\n",
    "CHECKPOINT_DIR = r\"D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\"\n",
    "if os.path.exists(os.path.join(CHECKPOINT_DIR, \"model.safetensors\")):\n",
    "    print(f\"âœ” Found checkpoint at: {CHECKPOINT_DIR}\")\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)\n",
    "    trainer = Trainer(model=model)\n",
    "    print(\"âœ” Model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"! ERROR: Checkpoint not found at '{CHECKPOINT_DIR}'. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION (FIXED) ---\n",
    "print(\"\\n===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    print(f\"Loading document test data from {DOC_BLIND_TEST_PATH}...\")\n",
    "    doc_test_df = pd.read_csv(DOC_BLIND_TEST_PATH)\n",
    "    doc_test_df.dropna(subset=['ID', 'Document'], inplace=True)\n",
    "    \n",
    "    print(\"Processing documents: tokenizing into sentences using pyarabic...\")\n",
    "    all_sentences, doc_ids = [], []\n",
    "    for _, row in doc_test_df.iterrows():\n",
    "        doc_id = row['ID']\n",
    "        full_document_text = row['Document']\n",
    "        \n",
    "        if isinstance(full_document_text, str) and full_document_text.strip():\n",
    "            # --- FIX: Use pyarabic.araby.sentence_tokenize ---\n",
    "            sentences_list = araby.sentence_tokenize(full_document_text)\n",
    "            \n",
    "            if sentences_list:\n",
    "                all_sentences.extend(sentences_list)\n",
    "                doc_ids.extend([doc_id] * len(sentences_list))\n",
    "        else:\n",
    "            print(f\"Warning: Document ID {doc_id} has empty or invalid text. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not all_sentences:\n",
    "        print(\"\\n! ERROR: No sentences were extracted. Check the 'Document' column in your CSV.\")\n",
    "        exit()\n",
    "\n",
    "    sentence_df = pd.DataFrame({'doc_id': doc_ids, 'sentence_text': all_sentences})\n",
    "    print(f\"Successfully created {len(sentence_df):,} sentences from {len(doc_test_df):,} documents.\")\n",
    "\n",
    "    print(\"\\nPreprocessing all sentences to D3Tok format (this may take a moment)...\")\n",
    "    sentence_df['processed_text'] = sentence_df['sentence_text'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "    \n",
    "    print(\"Generating predictions for all sentences...\")\n",
    "    test_dataset = ReadabilityDataset(sentence_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    sentence_df['raw_prediction'] = predictions.predictions.flatten()\n",
    "    \n",
    "    print(\"Aggregating results: finding the max readability score per document...\")\n",
    "    doc_predictions = sentence_df.groupby('doc_id')['raw_prediction'].max()\n",
    "    \n",
    "    rounded_preds = np.round(doc_predictions.values)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({'Sentence ID': doc_test_df['ID']})\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Sentence ID': doc_predictions.index, \n",
    "        'Prediction': (clipped_preds + 1).astype(int)\n",
    "    })\n",
    "    final_submission_df = final_submission_df.merge(pred_df, on='Sentence ID', how='left')\n",
    "    final_submission_df['Prediction'].fillna(1, inplace=True)\n",
    "    final_submission_df['Prediction'] = final_submission_df['Prediction'].astype(int)\n",
    "\n",
    "    print(f\"\\nSaving prediction file to: {SUBMISSION_PATH}\")\n",
    "    final_submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"Compressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "        \n",
    "    print(f\"âœ” Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'doc_blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final document prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330908e9-db85-42d9-958d-6e397b537a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b05635-8619-41ac-98b5-7b1fbb07c4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3599c767-c6e7-4780-bed8-90b943894bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CAMeL Tools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Tools initialized.\n",
      "\n",
      "===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\n",
      "\n",
      "âœ” Found checkpoint at: D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\n",
      "Loading model from checkpoint...\n",
      "âœ” Model loaded successfully.\n",
      "\n",
      "===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Loading document test data from .\\data\\doc_blind_test_data.csv...\n",
      "Processing documents: splitting into sentences by newline characters...\n",
      "\n",
      "--- Saving split sentences for manual review ---\n",
      "âœ” Sentences saved to review_split_sentences.csv\n",
      "\n",
      "Successfully created 100 sentences from 100 documents.\n",
      "\n",
      "Preprocessing all sentences to D3Tok format (this may take a moment)...\n",
      "Generating predictions for all sentences...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating results: finding the max readability score per document...\n",
      "\n",
      "Saving prediction file to: .\\submission\\submission_document_regression_final.csv\n",
      "Compressing submission_document_regression_final.csv into submission_document_regression_final.zip...\n",
      "âœ” Submission file submission_document_regression_final.zip created successfully.\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatima\\AppData\\Local\\Temp\\ipykernel_14292\\2229807753.py:150: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_submission_df['Prediction'].fillna(1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "# --- pyarabic is no longer needed ---\n",
    "\n",
    "# --- These camel-tools imports are correct and unchanged ---\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "DOC_BLIND_TEST_PATH = os.path.join(DATA_DIR, 'doc_blind_test_data.csv') \n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.zip\")\n",
    "\n",
    "\n",
    "# --- DATA PREPROCESSING (UNCHANGED) ---\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else: d3tok_forms.append(disambig_word.word)\n",
    "        else: d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n",
    "\n",
    "# --- Initialize Tools ---\n",
    "print(\"Initializing CAMeL Tools...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"âœ” Tools initialized.\")\n",
    "\n",
    "\n",
    "# --- DATASET CLASS (UNCHANGED) ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "\n",
    "# --- MODEL LOADING (UNCHANGED) ---\n",
    "print(\"\\n===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\\n\")\n",
    "CHECKPOINT_DIR = r\"D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\"\n",
    "if os.path.exists(os.path.join(CHECKPOINT_DIR, \"model.safetensors\")):\n",
    "    print(f\"âœ” Found checkpoint at: {CHECKPOINT_DIR}\")\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)\n",
    "    trainer = Trainer(model=model)\n",
    "    print(\"âœ” Model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"! ERROR: Checkpoint not found at '{CHECKPOINT_DIR}'. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION ---\n",
    "print(\"\\n===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    print(f\"Loading document test data from {DOC_BLIND_TEST_PATH}...\")\n",
    "    doc_test_df = pd.read_csv(DOC_BLIND_TEST_PATH)\n",
    "    doc_test_df.dropna(subset=['ID', 'Document'], inplace=True)\n",
    "    \n",
    "    print(\"Processing documents: splitting into sentences by newline characters...\")\n",
    "    all_sentences, doc_ids = [], []\n",
    "    for _, row in doc_test_df.iterrows():\n",
    "        doc_id = row['ID']\n",
    "        full_document_text = row['Document']\n",
    "        \n",
    "        if isinstance(full_document_text, str) and full_document_text.strip():\n",
    "            # --- CHANGE: Use the more robust split-by-newline method ---\n",
    "            sentences_list = full_document_text.split('\\n')\n",
    "            \n",
    "            # Clean up any empty strings that result from multiple newlines\n",
    "            sentences_list = [s.strip() for s in sentences_list if s.strip()]\n",
    "\n",
    "            if sentences_list:\n",
    "                all_sentences.extend(sentences_list)\n",
    "                doc_ids.extend([doc_id] * len(sentences_list))\n",
    "        else:\n",
    "            print(f\"Warning: Document ID {doc_id} has empty or invalid text. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not all_sentences:\n",
    "        print(\"\\n! ERROR: No sentences were extracted. Check the 'Document' column in your CSV.\")\n",
    "        exit()\n",
    "\n",
    "    sentence_df = pd.DataFrame({'doc_id': doc_ids, 'sentence_text': all_sentences})\n",
    "    \n",
    "    # --- NEW: Save the created sentences for you to review ---\n",
    "    print(\"\\n--- Saving split sentences for manual review ---\")\n",
    "    review_path = 'review_split_sentences.csv'\n",
    "    # Use utf-8 encoding to correctly save Arabic characters\n",
    "    sentence_df.to_csv(review_path, index=False, encoding='utf-8')\n",
    "    print(f\"âœ” Sentences saved to {review_path}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully created {len(sentence_df):,} sentences from {len(doc_test_df):,} documents.\")\n",
    "\n",
    "    print(\"\\nPreprocessing all sentences to D3Tok format (this may take a moment)...\")\n",
    "    sentence_df['processed_text'] = sentence_df['sentence_text'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "    \n",
    "    print(\"Generating predictions for all sentences...\")\n",
    "    test_dataset = ReadabilityDataset(sentence_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    sentence_df['raw_prediction'] = predictions.predictions.flatten()\n",
    "    \n",
    "    print(\"Aggregating results: finding the max readability score per document...\")\n",
    "    doc_predictions = sentence_df.groupby('doc_id')['raw_prediction'].max()\n",
    "    \n",
    "    rounded_preds = np.round(doc_predictions.values)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({'Sentence ID': doc_test_df['ID']})\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Sentence ID': doc_predictions.index, \n",
    "        'Prediction': (clipped_preds + 1).astype(int)\n",
    "    })\n",
    "    # --- BUG FIX: Corrected variable name ---\n",
    "    final_submission_df = final_submission_df.merge(pred_df, on='Sentence ID', how='left')\n",
    "    final_submission_df['Prediction'].fillna(1, inplace=True)\n",
    "    final_submission_df['Prediction'] = final_submission_df['Prediction'].astype(int)\n",
    "\n",
    "    print(f\"\\nSaving prediction file to: {SUBMISSION_PATH}\")\n",
    "    final_submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"Compressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "        \n",
    "    print(f\"âœ” Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'doc_blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final document prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a710c-b225-4e35-908c-f34e672a1a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6e403c-be67-4d77-a550-f0160aa82355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cachetools==4.2.4\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: cachetools\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 6.1.0\n",
      "    Uninstalling cachetools-6.1.0:\n",
      "      Successfully uninstalled cachetools-6.1.0\n",
      "Successfully installed cachetools-4.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cachetools==4.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bf8e02-1fe2-4d72-a181-8939f3939b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting camel_tools.tokenizers ---\n",
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'word']\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import camel_tools\n",
    "import camel_tools.tokenizers\n",
    "\n",
    "# This will print out all the available modules in the tokenizers directory\n",
    "# for your specific version of camel-tools.\n",
    "print(\"--- Inspecting camel_tools.tokenizers ---\")\n",
    "print(dir(camel_tools.tokenizers))\n",
    "print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6bb651c-d059-4fa8-9a1a-972b67c1c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarabic in c:\\users\\fatima\\appdata\\roaming\\python\\python310\\site-packages (0.6.15)\n",
      "Collecting Tashaphyne\n",
      "  Downloading Tashaphyne-0.3.6-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\programdata\\anaconda3\\envs\\barec_env\\lib\\site-packages (from pyarabic) (1.17.0)\n",
      "Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
      "Installing collected packages: Tashaphyne\n",
      "Successfully installed Tashaphyne-0.3.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarabic Tashaphyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766fc4fd-18c0-4cf7-9890-cb49e2a2cfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\barec_env\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\envs\\barec_env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\barec_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\barec_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 868.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 932.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 925.8 kB/s eta 0:00:00\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/2 [click]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ee305-6a4f-4f87-935b-3f8430bda20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002b752-d778-49b5-bb2a-0585a7799185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760338df-7c53-43d5-a344-4df3069a87ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CAMeL Tools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Tools initialized.\n",
      "\n",
      "===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\n",
      "\n",
      "âœ” Found checkpoint at: D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\n",
      "Loading model from checkpoint...\n",
      "âœ” Model loaded successfully.\n",
      "\n",
      "===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Loading document test data from .\\data\\doc_blind_test_data.csv...\n",
      "Processing documents: splitting into sentences by newline characters...\n",
      "\n",
      "âœ” Raw split sentences saved to review_split_sentences.csv\n",
      "Successfully created 3,420 sentences from 100 documents.\n",
      "\n",
      "Preprocessing all sentences to D3Tok format (this may take a moment)...\n",
      "âœ” D3tok processed output saved to review_d3tok_processed_output.csv\n",
      "\n",
      "Generating predictions for all sentences...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating results: finding the max readability score per document...\n",
      "\n",
      "Saving prediction file to: .\\submission\\submission_document_regression_final.csv\n",
      "Compressing submission_document_regression_final.csv into submission_document_regression_final.zip...\n",
      "âœ” Submission file submission_document_regression_final.zip created successfully.\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatima\\AppData\\Local\\Temp\\ipykernel_14292\\2168431033.py:147: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_submission_df['Prediction'].fillna(1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "DOC_BLIND_TEST_PATH = os.path.join(DATA_DIR, 'doc_blind_test_data.csv') \n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_document_regression_final.zip\")\n",
    "\n",
    "\n",
    "# --- DATA PREPROCESSING (UNCHANGED) ---\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    if not isinstance(text, str) or not text.strip(): return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else: d3tok_forms.append(disambig_word.word)\n",
    "        else: d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n",
    "\n",
    "# --- Initialize Tools ---\n",
    "print(\"Initializing CAMeL Tools...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"âœ” Tools initialized.\")\n",
    "\n",
    "\n",
    "# --- DATASET CLASS (UNCHANGED) ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "\n",
    "# --- MODEL LOADING (UNCHANGED) ---\n",
    "print(\"\\n===== LOADING PRE-TRAINED SENTENCE-LEVEL MODEL =====\\n\")\n",
    "CHECKPOINT_DIR = r\"D:\\arabic_readability_project\\results\\regression_readability-arabertv2-d3tok-reg\\checkpoint-10284\"\n",
    "if os.path.exists(os.path.join(CHECKPOINT_DIR, \"model.safetensors\")):\n",
    "    print(f\"âœ” Found checkpoint at: {CHECKPOINT_DIR}\")\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_DIR)\n",
    "    trainer = Trainer(model=model)\n",
    "    print(\"âœ” Model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"! ERROR: Checkpoint not found at '{CHECKPOINT_DIR}'. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION ---\n",
    "print(\"\\n===== DOCUMENT-LEVEL FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    print(f\"Loading document test data from {DOC_BLIND_TEST_PATH}...\")\n",
    "    doc_test_df = pd.read_csv(DOC_BLIND_TEST_PATH)\n",
    "    # --- CHANGE: Drop rows if 'Sentences' column is empty, not 'Document' ---\n",
    "    doc_test_df.dropna(subset=['ID', 'Sentences'], inplace=True)\n",
    "    \n",
    "    print(\"Processing documents: splitting into sentences by newline characters...\")\n",
    "    all_sentences, doc_ids = [], []\n",
    "    for _, row in doc_test_df.iterrows():\n",
    "        doc_id = row['ID']\n",
    "        # --- THE CRITICAL FIX: Read text from the 'Sentences' column ---\n",
    "        full_document_text = row['Sentences']\n",
    "        \n",
    "        if isinstance(full_document_text, str) and full_document_text.strip():\n",
    "            sentences_list = full_document_text.split('\\n')\n",
    "            sentences_list = [s.strip() for s in sentences_list if s.strip()]\n",
    "            if sentences_list:\n",
    "                all_sentences.extend(sentences_list)\n",
    "                doc_ids.extend([doc_id] * len(sentences_list))\n",
    "        else:\n",
    "            print(f\"Warning: Document ID {doc_id} has empty or invalid text in 'Sentences' column. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not all_sentences:\n",
    "        print(\"\\n! ERROR: No sentences were extracted. Check the 'Sentences' column in your CSV.\")\n",
    "        exit()\n",
    "\n",
    "    sentence_df = pd.DataFrame({'doc_id': doc_ids, 'sentence_text': all_sentences})\n",
    "    \n",
    "    # Save split sentences for review\n",
    "    review_split_path = 'review_split_sentences.csv'\n",
    "    sentence_df.to_csv(review_split_path, index=False, encoding='utf-8-sig') # Use utf-8-sig for Excel\n",
    "    print(f\"\\nâœ” Raw split sentences saved to {review_split_path}\")\n",
    "    print(f\"Successfully created {len(sentence_df):,} sentences from {len(doc_test_df):,} documents.\")\n",
    "\n",
    "    print(\"\\nPreprocessing all sentences to D3Tok format (this may take a moment)...\")\n",
    "    sentence_df['processed_text'] = sentence_df['sentence_text'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "    \n",
    "    # Save D3tok output for review\n",
    "    review_d3tok_path = 'review_d3tok_processed_output.csv'\n",
    "    sentence_df[['sentence_text', 'processed_text']].to_csv(review_d3tok_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ” D3tok processed output saved to {review_d3tok_path}\")\n",
    "    \n",
    "    print(\"\\nGenerating predictions for all sentences...\")\n",
    "    test_dataset = ReadabilityDataset(sentence_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    sentence_df['raw_prediction'] = predictions.predictions.flatten()\n",
    "    \n",
    "    print(\"Aggregating results: finding the max readability score per document...\")\n",
    "    doc_predictions = sentence_df.groupby('doc_id')['raw_prediction'].max()\n",
    "    \n",
    "    rounded_preds = np.round(doc_predictions.values)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({'Sentence ID': doc_test_df['ID']})\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Sentence ID': doc_predictions.index, \n",
    "        'Prediction': (clipped_preds + 1).astype(int)\n",
    "    })\n",
    "    final_submission_df = final_submission_df.merge(pred_df, on='Sentence ID', how='left')\n",
    "    final_submission_df['Prediction'].fillna(1, inplace=True)\n",
    "    final_submission_df['Prediction'] = final_submission_df['Prediction'].astype(int)\n",
    "\n",
    "    print(f\"\\nSaving prediction file to: {SUBMISSION_PATH}\")\n",
    "    final_submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    \n",
    "    print(f\"Compressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "        \n",
    "    print(f\"âœ” Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found at '{DOC_BLIND_TEST_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final document prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5770bce-3491-45d6-9a4a-ca0d89e832dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74011e0e-abfc-4600-98c6-81c154b13457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (barec_env)",
   "language": "python",
   "name": "barec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
