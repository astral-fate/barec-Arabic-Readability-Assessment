{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289341ee-f06f-4c1a-a353-4b9429d69cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Files loaded successfully.\n",
      "Processed the first DataFrame (dares).\n",
      "Columns selected and renamed: ['text', 'label']\n",
      "Processed the second DataFrame (train).\n",
      "Columns selected: ['text', 'label']\n",
      "\n",
      "DataFrames merged successfully.\n",
      "Total rows in new DataFrame: 68715\n",
      "\n",
      "üéâ Success! Merged file has been saved to:\n",
      "D:\\arabic_readability_project\\data\\strict\\prepros\\merged_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define the file paths for your input and output files\n",
    "#    Using raw strings (r'...') is recommended for Windows paths.\n",
    "dares_file_path = r'D:\\arabic_readability_project\\data\\dares\\dares_d3tok_processed_FULL.csv'\n",
    "train_file_path = r'D:\\arabic_readability_project\\data\\strict\\prepros\\train_preprocessed.csv'\n",
    "output_file_path = r'D:\\arabic_readability_project\\data\\strict\\prepros\\merged_data.csv'\n",
    "\n",
    "# 2. Load both CSV files into pandas DataFrames\n",
    "try:\n",
    "    df_dares = pd.read_csv(dares_file_path)\n",
    "    df_train = pd.read_csv(train_file_path)\n",
    "    print(\"‚úÖ Files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: File not found. Please check the path: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 3. Prepare the first DataFrame (dares)\n",
    "#    - Select only the 'd3tok_text' and 'label' columns.\n",
    "#    - Rename 'd3tok_text' to 'text' to match the second DataFrame.\n",
    "df_dares_subset = df_dares[['d3tok_text', 'label']].rename(columns={'d3tok_text': 'text'})\n",
    "print(\"Processed the first DataFrame (dares).\")\n",
    "print(f\"Columns selected and renamed: {df_dares_subset.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# 4. Prepare the second DataFrame (train)\n",
    "#    - Select the 'text' and 'label' columns to ensure the correct order.\n",
    "df_train_subset = df_train[['text', 'label']]\n",
    "print(\"Processed the second DataFrame (train).\")\n",
    "print(f\"Columns selected: {df_train_subset.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# 5. Concatenate (merge) the two DataFrames one under the other\n",
    "#    The data from df_dares_subset will be appended to the end of df_train_subset.\n",
    "#    ignore_index=True resets the index of the new DataFrame.\n",
    "merged_df = pd.concat([df_train_subset, df_dares_subset], ignore_index=True)\n",
    "print(\"\\nDataFrames merged successfully.\")\n",
    "print(f\"Total rows in new DataFrame: {len(merged_df)}\")\n",
    "\n",
    "\n",
    "# 6. Save the final merged DataFrame to a new CSV file\n",
    "#    - index=False prevents pandas from writing the DataFrame index as a column.\n",
    "#    - encoding='utf-8-sig' is important for compatibility with Arabic text, especially in Excel.\n",
    "merged_df.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nüéâ Success! Merged file has been saved to:\\n{output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51523c3-face-411f-b946-c1ad247b43f7",
   "metadata": {},
   "source": [
    "# split dev to contain dares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72ee69b-b33d-49c1-bbff-de81e567e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Augmentation Process ---\n",
      "Loading merged data from 'D:/arabic_readability_project/data/strict/prepros/merged_data.csv'...\n",
      "Successfully loaded 68715 total records.\n",
      "Loading original development data from 'D:/arabic_readability_project/data/strict/prepros/dev_preprocessed.csv'...\n",
      "Successfully loaded 7310 original dev records.\n",
      "\n",
      "Identified 54847 BAREC records and 13868 DARES records in the merged file.\n",
      "\n",
      "Performing stratified split on the DARES data to select 15.0% for the dev set...\n",
      "‚úî DARES data split successfully!\n",
      "  - 2081 records will be added to the dev set.\n",
      "  - 11787 records will remain in the training set.\n",
      "\n",
      "--- Saving New Augmented Datasets ---\n",
      "New training set saved to 'D:/arabic_readability_project/data/strict/prepros/train_augmented.csv' (66634 records)\n",
      "New development set saved to 'D:/arabic_readability_project/data/strict/prepros/dev_augmented.csv' (9391 records)\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to your merged data file (BAREC + DARES)\n",
    "MERGED_DATA_PATH = 'D:/arabic_readability_project/data/strict/prepros/merged_data.csv' \n",
    "\n",
    "# Define the path to your original, untouched development file (BAREC only)\n",
    "ORIGINAL_DEV_PATH = 'D:/arabic_readability_project/data/strict/prepros/dev_preprocessed.csv'\n",
    "\n",
    "# Define the names for your new, augmented output files\n",
    "NEW_TRAIN_CSV_PATH = 'D:/arabic_readability_project/data/strict/prepros/train_augmented.csv'\n",
    "NEW_DEV_CSV_PATH = 'D:/arabic_readability_project/data/strict/prepros/dev_augmented.csv'\n",
    "\n",
    "# Define the row index where the DARES dataset begins in the merged file\n",
    "DARES_START_INDEX = 54847\n",
    "\n",
    "# Define the column to stratify on.\n",
    "STRATIFY_COLUMN = 'label' \n",
    "\n",
    "# Define the proportion of the DARES data to move to the development set\n",
    "DARES_DEV_SET_SIZE = 0.15\n",
    "\n",
    "# A random state for reproducibility of the split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Main Splitting Logic ---\n",
    "def create_augmented_split(merged_path, original_dev_path, new_train_path, new_dev_path, dares_start_index, stratify_col, test_size, random_state):\n",
    "    \"\"\"\n",
    "    Loads a merged dataset and an original dev set. It then moves a stratified\n",
    "    sample of the new data (DARES) from the training set to the dev set.\n",
    "\n",
    "    Args:\n",
    "        merged_path (str): Path to the merged (BAREC + DARES) data file.\n",
    "        original_dev_path (str): Path to the original development data file.\n",
    "        new_train_path (str): Path to save the new, augmented training data CSV.\n",
    "        new_dev_path (str): Path to save the new, augmented development data CSV.\n",
    "        dares_start_index (int): The index where the DARES data starts.\n",
    "        stratify_col (str): The name of the column to stratify the split by.\n",
    "        test_size (float): The proportion of the DARES dataset to move to the dev set.\n",
    "        random_state (int): Seed for the random number generator for reproducibility.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Data Augmentation Process ---\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    try:\n",
    "        print(f\"Loading merged data from '{merged_path}'...\")\n",
    "        merged_df = pd.read_csv(merged_path)\n",
    "        print(f\"Successfully loaded {len(merged_df)} total records.\")\n",
    "\n",
    "        print(f\"Loading original development data from '{original_dev_path}'...\")\n",
    "        original_dev_df = pd.read_csv(original_dev_path)\n",
    "        print(f\"Successfully loaded {len(original_dev_df)} original dev records.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find a required data file. {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV files: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Separate BAREC and DARES data from the merged file ---\n",
    "    barec_train_df = merged_df.loc[:dares_start_index - 1]\n",
    "    dares_df = merged_df.loc[dares_start_index:]\n",
    "    print(f\"\\nIdentified {len(barec_train_df)} BAREC records and {len(dares_df)} DARES records in the merged file.\")\n",
    "\n",
    "    # --- Stratified Split on DARES data ---\n",
    "    print(f\"\\nPerforming stratified split on the DARES data to select {test_size*100}% for the dev set...\")\n",
    "    \n",
    "    if dares_df.empty:\n",
    "        print(\"Warning: No DARES data to split. The dev set will not be augmented.\")\n",
    "        dares_to_dev = pd.DataFrame()\n",
    "        dares_to_train = pd.DataFrame()\n",
    "    else:\n",
    "        try:\n",
    "            # Separate features (X) and target (y) for splitting\n",
    "            X_dares = dares_df.drop(columns=[stratify_col])\n",
    "            y_dares = dares_df[stratify_col]\n",
    "\n",
    "            # Perform the stratified split\n",
    "            dares_to_train_X, dares_to_dev_X, dares_to_train_y, dares_to_dev_y = train_test_split(\n",
    "                X_dares, y_dares,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state,\n",
    "                stratify=y_dares\n",
    "            )\n",
    "\n",
    "            # Recombine into dataframes\n",
    "            dares_to_train = pd.concat([dares_to_train_X, dares_to_train_y], axis=1)\n",
    "            dares_to_dev = pd.concat([dares_to_dev_X, dares_to_dev_y], axis=1)\n",
    "            print(\"‚úî DARES data split successfully!\")\n",
    "            print(f\"  - {len(dares_to_dev)} records will be added to the dev set.\")\n",
    "            print(f\"  - {len(dares_to_train)} records will remain in the training set.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the split of DARES data: {e}\")\n",
    "            return\n",
    "\n",
    "    # --- Create New Augmented Datasets ---\n",
    "    # The new training set is the original BAREC part + the part of DARES that was not moved.\n",
    "    new_train_df = pd.concat([barec_train_df, dares_to_train], ignore_index=True)\n",
    "    \n",
    "    # The new dev set is the original BAREC dev set + the part of DARES that was moved.\n",
    "    new_dev_df = pd.concat([original_dev_df, dares_to_dev], ignore_index=True)\n",
    "\n",
    "    # --- Saving the New Files ---\n",
    "    print(\"\\n--- Saving New Augmented Datasets ---\")\n",
    "    try:\n",
    "        new_train_df.to_csv(new_train_path, index=False)\n",
    "        print(f\"New training set saved to '{new_train_path}' ({len(new_train_df)} records)\")\n",
    "        \n",
    "        new_dev_df.to_csv(new_dev_path, index=False)\n",
    "        print(f\"New development set saved to '{new_dev_path}' ({len(new_dev_df)} records)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving files: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Script Finished ---\")\n",
    "\n",
    "\n",
    "# --- Run the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    create_augmented_split(\n",
    "        merged_path=MERGED_DATA_PATH,\n",
    "        original_dev_path=ORIGINAL_DEV_PATH,\n",
    "        new_train_path=NEW_TRAIN_CSV_PATH,\n",
    "        new_dev_path=NEW_DEV_CSV_PATH,\n",
    "        dares_start_index=DARES_START_INDEX,\n",
    "        stratify_col=STRATIFY_COLUMN,\n",
    "        test_size=DARES_DEV_SET_SIZE,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325983e6-de07-48e2-9fa0-0da28aee55bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (barec_env)",
   "language": "python",
   "name": "barec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
