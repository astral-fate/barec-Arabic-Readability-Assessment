{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c018de-f862-4650-86bc-dd11203d6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc61c4d-1bce-4a70-8380-efefa0d222b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =====================================================================================\n",
    "# --- Model & Environment Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"results_dares_v3\", f\"regression_combined_{MODEL_NAME.split('/')[-1]}\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "BAREC_TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "BAREC_DEV_PATH = os.path.join(DATA_DIR, 'dev.csv')\n",
    "OSMAN_TRAIN_PATH = os.path.join(DATA_DIR, 'dares_train.csv')\n",
    "OSMAN_DEV_PATH = os.path.join(DATA_DIR, 'dares_dev.csv')\n",
    "BLIND_TEST_PATH = os.path.join(DATA_DIR, 'blind_test_data.csv')\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_regression_combined_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_regression_combined_final.zip\")\n",
    "TRAIN_PREPROCESSED_PATH = os.path.join(DATA_DIR, 'train_combined_preprocessed_d3tok.csv')\n",
    "DEV_PREPROCESSED_PATH = os.path.join(DATA_DIR, 'dev_combined_preprocessed_d3tok.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c0af0c-fa9a-4034-9924-42d9e6e89a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 2. D3Tok PREPROCESSING FUNCTION\n",
    "# =====================================================================================\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    \"\"\"\n",
    "    Preprocesses text into the D3Tok format using BERTUnfactoredDisambiguator.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else:\n",
    "                d3tok_forms.append(disambig_word.word)\n",
    "        else:\n",
    "            d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3473c-7e72-4cc7-a264-ae601aaea8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c0ee1-ef42-43e3-bc05-dae799534faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6245ab6-5f7c-4c70-a2d3-3083379aaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 3. DATA LOADING, MAPPING, AND COMBINING\n",
    "# =====================================================================================\n",
    "def load_or_preprocess_data(disambiguator):\n",
    "    \"\"\"\n",
    "    Loads preprocessed combined data if it exists. Otherwise, it loads BAREC and Osman\n",
    "    data, standardizes, maps, combines, and preprocesses them with D3Tok format.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and Preparing Combined Readability Data ---\")\n",
    "    if os.path.exists(TRAIN_PREPROCESSED_PATH) and os.path.exists(DEV_PREPROCESSED_PATH):\n",
    "        print(f\"✔ Found preprocessed combined files. Loading them from:\\n- {TRAIN_PREPROCESSED_PATH}\\n- {DEV_PREPROCESSED_PATH}\")\n",
    "        train_df = pd.read_csv(TRAIN_PREPROCESSED_PATH)\n",
    "        val_df = pd.read_csv(DEV_PREPROCESSED_PATH)\n",
    "        train_df['text'] = train_df['text'].astype(str)\n",
    "        val_df['text'] = val_df['text'].astype(str)\n",
    "        print(f\"✔ Successfully loaded {len(train_df)} training and {len(val_df)} validation records.\")\n",
    "        return train_df, val_df\n",
    "    else:\n",
    "        print(\"Preprocessed combined files not found. Starting one-time data integration and preprocessing...\")\n",
    "        try:\n",
    "            # --- Load BAREC Data (assumed to be comma-separated) ---\n",
    "            print(\"\\n1. Loading BAREC data...\")\n",
    "            barec_train_df = pd.read_csv(BAREC_TRAIN_PATH)\n",
    "            barec_val_df = pd.read_csv(BAREC_DEV_PATH)\n",
    "            barec_train_df.rename(columns={'Sentence': 'text', 'Readability_Level_19': 'label'}, inplace=True)\n",
    "            barec_val_df.rename(columns={'Sentence': 'text', 'Readability_Level_19': 'label'}, inplace=True)\n",
    "            print(f\"  - Loaded {len(barec_train_df)} BAREC training records.\")\n",
    "            print(f\"  - Loaded {len(barec_val_df)} BAREC validation records.\")\n",
    "\n",
    "            # --- Load and Map Osman Data ---\n",
    "            print(\"\\n2. Loading and mapping Osman data...\")\n",
    "            \n",
    "            # ===== FIX IS HERE =====\n",
    "            # The Osman files are separated by Tabs (\\t), not commas. We must specify this.\n",
    "            osman_train_df = pd.read_csv(OSMAN_TRAIN_PATH, sep='\\t')\n",
    "            osman_dev_df = pd.read_csv(OSMAN_DEV_PATH, sep='\\t')\n",
    "            # =======================\n",
    "\n",
    "            for df, name in [(osman_train_df, \"training\"), (osman_dev_df, \"validation\")]:\n",
    "                text_col = 'Text'\n",
    "                label_col = 'Fine-grained'\n",
    "\n",
    "                if text_col not in df.columns or label_col not in df.columns:\n",
    "                    raise ValueError(f\"Error in Osman {name} data: Columns '{text_col}' or '{label_col}' not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "                df.rename(columns={text_col: 'text', label_col: 'label'}, inplace=True)\n",
    "                df['label'] = df['label'].str.replace('G', '', regex=False).astype(int)\n",
    "                print(f\"  - Loaded and mapped {len(df)} Osman {name} records.\")\n",
    "\n",
    "            # --- Combine Datasets ---\n",
    "            print(\"\\n3. Combining BAREC and Osman datasets...\")\n",
    "            train_df = pd.concat([barec_train_df[['text', 'label']], osman_train_df[['text', 'label']]], ignore_index=True)\n",
    "            val_df = pd.concat([barec_val_df[['text', 'label']], osman_dev_df[['text', 'label']]], ignore_index=True)\n",
    "            print(f\"  - Combined training data size: {len(train_df)} records.\")\n",
    "            print(f\"  - Combined validation data size: {len(val_df)} records.\")\n",
    "\n",
    "            # --- Final Cleaning and Label Normalization ---\n",
    "            train_df.dropna(subset=['text', 'label'], inplace=True)\n",
    "            val_df.dropna(subset=['label', 'text'], inplace=True)\n",
    "            train_df['text'] = train_df['text'].astype(str)\n",
    "            val_df['text'] = val_df['text'].astype(str)\n",
    "            \n",
    "            train_df['label'] = train_df['label'].astype(int) - 1\n",
    "            val_df['label'] = val_df['label'].astype(int) - 1\n",
    "            train_df['label'] = train_df['label'].astype(float)\n",
    "            val_df['label'] = val_df['label'].astype(float)\n",
    "            \n",
    "            print(\"\\n4. Preprocessing all text to D3Tok format (this will only run once)...\")\n",
    "            train_df['text'] = train_df['text'].apply(lambda x: preprocess_d3tok(x, disambiguator))\n",
    "            val_df['text'] = val_df['text'].apply(lambda x: preprocess_d3tok(x, disambiguator))\n",
    "            print(\"✔ Text preprocessing finished.\")\n",
    "\n",
    "            print(\"\\n5. Saving combined preprocessed data for future use...\")\n",
    "            train_df.to_csv(TRAIN_PREPROCESSED_PATH, index=False)\n",
    "            val_df.to_csv(DEV_PREPROCESSED_PATH, index=False)\n",
    "            print(f\"** Saved preprocessed files to {TRAIN_PREPROCESSED_PATH} and {DEV_PREPROCESSED_PATH} **\")\n",
    "            \n",
    "            return train_df, val_df\n",
    "\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            print(f\"\\n! SCRIPT STOPPED DUE TO AN ERROR: {e}\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"! An unexpected error occurred during initial data processing: {e}\")\n",
    "            return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e500a4-2bd2-473d-aefb-b4694434fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT Disambiguator for preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Combined Readability Data ---\n",
      "✔ Found preprocessed combined files. Loading them from:\n",
      "- .\\data\\train_combined_preprocessed_d3tok.csv\n",
      "- .\\data\\dev_combined_preprocessed_d3tok.csv\n",
      "✔ Successfully loaded 64548 training and 8690 validation records.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 4. INITIALIZATION AND DATASET PREPARATION\n",
    "# =====================================================================================\n",
    "print(\"Initializing BERT Disambiguator for preprocessing...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "\n",
    "train_df, val_df = load_or_preprocess_data(bert_disambiguator)\n",
    "\n",
    "if train_df is None:\n",
    "    print(\"Stopping script due to data loading failure.\")\n",
    "    exit()\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- PyTorch Dataset Class ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.flatten()\n",
    "    rounded_preds = np.round(preds)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1).astype(int)\n",
    "    labels = p.label_ids.astype(int)\n",
    "    qwk = cohen_kappa_score(labels, clipped_preds, weights='quadratic')\n",
    "    return {\"qwk\": qwk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f62e9d-d51e-4be9-827e-cf97c6235c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INITIALIZING REGRESSION MODEL AND TRAINER =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\barec_env\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting training from the beginning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68595' max='72630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68595/72630 4:08:02 < 14:35, 4.61 it/s, Epoch 17/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Qwk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.835900</td>\n",
       "      <td>3.675830</td>\n",
       "      <td>0.808287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.813600</td>\n",
       "      <td>3.827279</td>\n",
       "      <td>0.799210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.432100</td>\n",
       "      <td>3.964330</td>\n",
       "      <td>0.808837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.195100</td>\n",
       "      <td>3.875272</td>\n",
       "      <td>0.810657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.939700</td>\n",
       "      <td>3.864078</td>\n",
       "      <td>0.812162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>3.919899</td>\n",
       "      <td>0.812337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>3.933153</td>\n",
       "      <td>0.805120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>3.752745</td>\n",
       "      <td>0.812156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>3.832563</td>\n",
       "      <td>0.811683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>3.751179</td>\n",
       "      <td>0.818976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>3.807392</td>\n",
       "      <td>0.814611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>3.744505</td>\n",
       "      <td>0.820678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>3.722504</td>\n",
       "      <td>0.816447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>3.730352</td>\n",
       "      <td>0.818337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>3.726217</td>\n",
       "      <td>0.814629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>3.726240</td>\n",
       "      <td>0.817174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>3.733586</td>\n",
       "      <td>0.816124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Training finished.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 5. MODEL TRAINING\n",
    "# =====================================================================================\n",
    "print(\"\\n===== INITIALIZING REGRESSION MODEL AND TRAINER =====\\n\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "train_dataset = ReadabilityDataset(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "val_dataset = ReadabilityDataset(val_df['text'].tolist(), val_df['label'].tolist())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "\n",
    "# --- MODIFICATION FOR RESUMING ---\n",
    "# Check if a checkpoint exists in the output directory\n",
    "if os.path.isdir(CHECKPOINT_DIR):\n",
    "    # Find the latest checkpoint directory (e.g., 'checkpoint-4044')\n",
    "    checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-72630\")]\n",
    "    if checkpoints:\n",
    "        # Sort by step number to get the latest one\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "        print(f\"Resuming training from checkpoint: {latest_checkpoint_path}\")\n",
    "        trainer.train(resume_from_checkpoint=latest_checkpoint_path)\n",
    "    else:\n",
    "        # No checkpoints found, start training from scratch\n",
    "        print(\"No checkpoint found. Starting training from the beginning...\")\n",
    "        trainer.train()\n",
    "else:\n",
    "    # Output directory doesn't even exist, start fresh\n",
    "    print(\"No checkpoint directory found. Starting training from the beginning...\")\n",
    "    trainer.train()\n",
    "\n",
    "print(\"✔ Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801aaf31-b618-41af-8ef6-941ae22b9370",
   "metadata": {},
   "source": [
    "# test set result sentse \n",
    "Scores:\n",
    "{'accuracy': 48.8, 'accuracy+-1': 71.3, 'avg_abs_dist': 1.1, 'qwk': 83.9, 'accuracy_7': 62.5, 'accuracy_5': 67.6, 'accuracy_3': 74.3}\n",
    "\n",
    "\n",
    "# doc\n",
    "\n",
    "Scores:\n",
    "{'accuracy': 39.0, 'accuracy+-1': 87.0, 'avg_abs_dist': 0.8, 'qwk': 76.1, 'accuracy_7': 68.0, 'accuracy_5': 68.0, 'accuracy_3': 91.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfcf477-945a-4712-88ab-9f672e64a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Preprocessing blind test text to D3Tok format...\n",
      "Generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prediction file to: .\\submission\\submission_regression_combined_final.csv\n",
      "\n",
      "Compressing submission_regression_combined_final.csv into submission_regression_combined_final.zip...\n",
      "✔ Submission file submission_regression_combined_final.zip created successfully.\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 6. FINAL PREDICTION AND SUBMISSION\n",
    "# =====================================================================================\n",
    "print(\"\\n===== FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    test_df = pd.read_csv(BLIND_TEST_PATH)\n",
    "    test_df.dropna(subset=['Sentence'], inplace=True)\n",
    "\n",
    "    print(\"Preprocessing blind test text to D3Tok format...\")\n",
    "    # FIX 1: Use the correct variable name 'bert_disambiguator'\n",
    "    test_df['processed_text'] = test_df['Sentence'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "\n",
    "    print(\"Generating predictions on the test set...\")\n",
    "    test_dataset = ReadabilityDataset(test_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    raw_preds = predictions.predictions.flatten()\n",
    "    rounded_preds = np.round(raw_preds)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "\n",
    "    test_df['Prediction'] = (clipped_preds + 1).astype(int)\n",
    "\n",
    "    # FIX 2: Use the 'ID' column from the test file and rename it to 'Sentence ID' for the submission.\n",
    "    submission_df = test_df[['ID', 'Prediction']]\n",
    "    submission_df = submission_df.rename(columns={'ID': 'Sentence ID'})\n",
    "    \n",
    "    print(f\"Saving prediction file to: {SUBMISSION_PATH}\")\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    print(f\"\\nCompressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "\n",
    "    print(f\"✔ Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except KeyError:\n",
    "    # Add a more specific error message for this common problem\n",
    "    print(\"! KEY ERROR: Could not find the expected 'ID' column in 'blind_test_data.csv'. Please check the file's column names.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809428f-1b47-46cc-8efc-be53d52a25ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (barec_env)",
   "language": "python",
   "name": "barec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
