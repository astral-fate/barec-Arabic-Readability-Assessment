{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abf43cc-171c-4fe4-9e13-91499c04835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸš€ Starting Master Pre-processing Script for Local Jupyter ---\n",
      "\n",
      "[1/5] Initializing BERT Disambiguator and loading SAMER Lexicon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Models and lexicon loaded.\n",
      "\n",
      "[2/5] Loading and merging all BAREC and SAMER data...\n",
      "âœ”ï¸ Total combined records: 105184\n",
      "\n",
      "[3/5] Creating new 85%/15% stratified split...\n",
      "âœ”ï¸ Split complete.\n",
      "   - Final Training Set Size:   89406\n",
      "   - Final Validation Set Size: 15778\n",
      "\n",
      "[4/5] Applying D3Tok and Lexicon feature extraction to new splits...\n",
      "  > Processing Training data in batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8660677ae875437b8353d93fe3ef35e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Training Batches:   0%|          | 0/699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Saved Colab Training file to: .\\colab_data\\train_lex.csv\n",
      "  > Processing Validation data in batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a22ce9d78148f680bf8f40b7f4055a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Validation Batches:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Saved Colab Validation file to: .\\colab_data\\dev_lex.csv\n",
      "\n",
      "[5/5] Processing the blind test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab174e912a2407693356c768c670759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Test Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Saved Colab test file to: .\\colab_data\\test_lex.csv\n",
      "\n",
      "--- ðŸŽ‰ All files are processed and saved in the 'colab_data' folder. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm # Import the progress bar library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =====================================================================================\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"colab_data\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Input File Paths (Raw Data) ---\n",
    "BAREC_TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "BAREC_DEV_PATH = os.path.join(DATA_DIR, 'dev.csv')\n",
    "BLIND_TEST_PATH = os.path.join(DATA_DIR, 'blind_test_data.csv')\n",
    "SAMER_CORPUS_PATH = os.path.join(DATA_DIR, 'samer_train.tsv')\n",
    "SAMER_LEXICON_PATH = os.path.join(DATA_DIR, 'samer_lexicon.tsv')\n",
    "\n",
    "# --- Output File Paths (Colab-Ready Data) ---\n",
    "TRAIN_OUTPUT_PATH = os.path.join(OUTPUT_DIR, 'train_lex.csv')\n",
    "DEV_OUTPUT_PATH = os.path.join(OUTPUT_DIR, 'dev_lex.csv')\n",
    "TEST_OUTPUT_PATH = os.path.join(OUTPUT_DIR, 'test_lex.csv')\n",
    "\n",
    "# --- Splitting & Processing Parameters ---\n",
    "VALIDATION_SPLIT_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "# BATCH_SIZE controls how many sentences are processed at once.\n",
    "# A larger size is often more efficient for the CPU.\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# =====================================================================================\n",
    "# 2. HELPER FUNCTION (Processes a batch of sentences)\n",
    "# =====================================================================================\n",
    "def process_batch(batch_texts, disambiguator, lexicon_map):\n",
    "    \"\"\"Processes a batch of sentences to extract D3Tok and lexical features.\"\"\"\n",
    "    valid_texts = [text for text in batch_texts if isinstance(text, str) and text.strip()]\n",
    "    if not valid_texts:\n",
    "        return [(\"\", 0.0, 0.0)] * len(batch_texts)\n",
    "\n",
    "    tokenized_sentences = [simple_word_tokenize(text) for text in valid_texts]\n",
    "    disambiguated_batch = disambiguator.disambiguate_sentences(tokenized_sentences)\n",
    "    \n",
    "    results = []\n",
    "    for disambiguated_sentence in disambiguated_batch:\n",
    "        d3tok_forms = []\n",
    "        for da in disambiguated_sentence:\n",
    "            if da.analyses and 'd3tok' in da.analyses[0][1]:\n",
    "                d3tok = dediac_ar(da.analyses[0][1]['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else:\n",
    "                d3tok_forms.append(da.word)\n",
    "        d3tok_text = \" \".join(d3tok_forms)\n",
    "\n",
    "        scores = []\n",
    "        for dw in disambiguated_sentence:\n",
    "            if dw.analyses:\n",
    "                analysis = dw.analyses[0][1]\n",
    "                lemma, pos = analysis.get('lex'), analysis.get('pos')\n",
    "                if lemma and pos:\n",
    "                    score = lexicon_map.get(f\"{dediac_ar(lemma)}#{pos}\")\n",
    "                    if score is not None:\n",
    "                        scores.append(score)\n",
    "        \n",
    "        avg_readability = np.mean(scores) if scores else 0.0\n",
    "        max_readability = np.max(scores) if scores else 0.0\n",
    "        results.append((d3tok_text, avg_readability, max_readability))\n",
    "    return results\n",
    "\n",
    "# =====================================================================================\n",
    "# 3. MAIN SCRIPT EXECUTION\n",
    "# =====================================================================================\n",
    "def main():\n",
    "    \"\"\"Main function to perform all pre-processing in the correct order.\"\"\"\n",
    "    print(\"--- ðŸš€ Starting Master Pre-processing Script for Local Jupyter ---\")\n",
    "\n",
    "    # --- Step 1: Initialize Models & Load Lexicon (CPU Only) ---\n",
    "    print(\"\\n[1/5] Initializing BERT Disambiguator and loading SAMER Lexicon...\")\n",
    "    try:\n",
    "        # Load the models normally; they will run on the CPU.\n",
    "        disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "        lexicon_df = pd.read_csv(SAMER_LEXICON_PATH, sep='\\t')\n",
    "        lexicon_map = lexicon_df.set_index('lemma#pos')['readability (rounded average)'].to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR: Could not load required models or files: {e}\")\n",
    "        return\n",
    "    print(\"âœ”ï¸ Models and lexicon loaded.\")\n",
    "\n",
    "    # --- Step 2: Load and Merge Data (Unchanged) ---\n",
    "    print(\"\\n[2/5] Loading and merging all BAREC and SAMER data...\")\n",
    "    try:\n",
    "        barec_train_df = pd.read_csv(BAREC_TRAIN_PATH)[['Sentence', 'Readability_Level_19']]\n",
    "        barec_dev_df = pd.read_csv(BAREC_DEV_PATH)[['Sentence', 'Readability_Level_19']]\n",
    "        samer_df = pd.read_csv(SAMER_CORPUS_PATH, sep='\\t')\n",
    "        samer_records = []\n",
    "        for level_name, barec_level in {'L3': 4, 'L4': 10, 'L5': 16}.items():\n",
    "            samer_subset = samer_df[[level_name]].dropna().rename(columns={level_name: 'Sentence'})\n",
    "            samer_subset['Readability_Level_19'] = barec_level\n",
    "            samer_records.append(samer_subset)\n",
    "        samer_augmentation_df = pd.concat(samer_records, ignore_index=True)\n",
    "        combined_df = pd.concat([barec_train_df, barec_dev_df, samer_augmentation_df], ignore_index=True)\n",
    "        combined_df.dropna(inplace=True)\n",
    "        combined_df = combined_df.rename(columns={'Sentence': 'text', 'Readability_Level_19': 'label'})\n",
    "        print(f\"âœ”ï¸ Total combined records: {len(combined_df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR loading source data: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Step 3: Stratified Split (Unchanged) ---\n",
    "    print(\"\\n[3/5] Creating new 85%/15% stratified split...\")\n",
    "    final_train_df, final_dev_df = train_test_split(\n",
    "        combined_df, test_size=VALIDATION_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=combined_df['label']\n",
    "    )\n",
    "    print(\"âœ”ï¸ Split complete.\")\n",
    "    print(f\"   - Final Training Set Size:   {len(final_train_df)}\")\n",
    "    print(f\"   - Final Validation Set Size: {len(final_dev_df)}\")\n",
    "\n",
    "    # --- Step 4: Process Splits with Progress Bar ---\n",
    "    print(\"\\n[4/5] Applying D3Tok and Lexicon feature extraction to new splits...\")\n",
    "    for name, df in [('Training', final_train_df), ('Validation', final_dev_df)]:\n",
    "        print(f\"  > Processing {name} data in batches...\")\n",
    "        all_results = []\n",
    "        \n",
    "        # THIS IS THE LOGGING LOOP\n",
    "        # tqdm will create a progress bar showing (current_batch/total_batches)\n",
    "        # For example: 13/2794 [00:10<1:30:00, 2.5 it/s]\n",
    "        for i in tqdm(range(0, len(df), BATCH_SIZE), desc=f\"Processing {name} Batches\"):\n",
    "            batch_texts = df['text'].iloc[i:i+BATCH_SIZE].tolist()\n",
    "            batch_results = process_batch(batch_texts, disambiguator, lexicon_map)\n",
    "            all_results.extend(batch_results)\n",
    "\n",
    "        processed_df = pd.DataFrame(all_results, columns=['d3tok_text', 'avg_readability', 'max_readability'], index=df.index)\n",
    "        df[['d3tok_text', 'avg_readability', 'max_readability']] = processed_df\n",
    "        df['label'] = (df['label'].astype(int) - 1)\n",
    "        \n",
    "        output_path = TRAIN_OUTPUT_PATH if name == 'Training' else DEV_OUTPUT_PATH\n",
    "        df[['label', 'd3tok_text', 'avg_readability', 'max_readability']].to_csv(output_path, index=False)\n",
    "        print(f\"âœ”ï¸ Saved Colab {name} file to: {output_path}\")\n",
    "\n",
    "    # --- Step 5: Process Blind Test Set with Progress Bar ---\n",
    "    print(\"\\n[5/5] Processing the blind test set...\")\n",
    "    try:\n",
    "        test_df = pd.read_csv(BLIND_TEST_PATH).dropna(subset=['Sentence'])\n",
    "        all_test_results = []\n",
    "\n",
    "        # Logging loop for the test set\n",
    "        for i in tqdm(range(0, len(test_df), BATCH_SIZE), desc=\"Processing Test Batches\"):\n",
    "            batch_texts = test_df['Sentence'].iloc[i:i+BATCH_SIZE].tolist()\n",
    "            batch_results = process_batch(batch_texts, disambiguator, lexicon_map)\n",
    "            all_test_results.extend(batch_results)\n",
    "\n",
    "        processed_test_df = pd.DataFrame(all_test_results, columns=['d3tok_text', 'avg_readability', 'max_readability'], index=test_df.index)\n",
    "        test_df[['d3tok_text', 'avg_readability', 'max_readability']] = processed_test_df\n",
    "        test_df[['ID', 'd3tok_text', 'avg_readability', 'max_readability']].to_csv(TEST_OUTPUT_PATH, index=False)\n",
    "        print(f\"âœ”ï¸ Saved Colab test file to: {TEST_OUTPUT_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR processing test data: {e}\")\n",
    "\n",
    "    print(\"\\n--- ðŸŽ‰ All files are processed and saved in the 'colab_data' folder. ---\")\n",
    "\n",
    "# In the next cell, run this to start the process:\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b428f-c500-4171-b42d-a61bcd564034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07516923-7115-4536-99e2-7041bf32da97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (barec_env)",
   "language": "python",
   "name": "barec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
