{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c018de-f862-4650-86bc-dd11203d6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc61c4d-1bce-4a70-8380-efefa0d222b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =====================================================================================\n",
    "# --- Model & Environment Configuration ---\n",
    "MODEL_NAME = \"CAMeL-Lab/readability-arabertv2-d3tok-reg\"\n",
    "NUM_LABELS = 1\n",
    "TARGET_CLASSES = 19\n",
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"results_dares_v2\", f\"regression_combined_{MODEL_NAME.split('/')[-1]}\")\n",
    "SUBMISSION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "BAREC_TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "BAREC_DEV_PATH = os.path.join(DATA_DIR, 'dev.csv')\n",
    "OSMAN_TRAIN_PATH = os.path.join(DATA_DIR, 'dares_train.csv')\n",
    "OSMAN_DEV_PATH = os.path.join(DATA_DIR, 'dares_dev.csv')\n",
    "BLIND_TEST_PATH = os.path.join(DATA_DIR, 'blind_test_data.csv')\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_regression_combined_final.csv\")\n",
    "ZIPPED_SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission_regression_combined_final.zip\")\n",
    "TRAIN_PREPROCESSED_PATH = os.path.join(DATA_DIR, 'train_combined_preprocessed_d3tok.csv')\n",
    "DEV_PREPROCESSED_PATH = os.path.join(DATA_DIR, 'dev_combined_preprocessed_d3tok.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c0af0c-fa9a-4034-9924-42d9e6e89a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 2. D3Tok PREPROCESSING FUNCTION\n",
    "# =====================================================================================\n",
    "def preprocess_d3tok(text, disambiguator):\n",
    "    \"\"\"\n",
    "    Preprocesses text into the D3Tok format using BERTUnfactoredDisambiguator.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    disambiguated_sentence = disambiguator.disambiguate(tokens)\n",
    "    d3tok_forms = []\n",
    "    for disambig_word in disambiguated_sentence:\n",
    "        if disambig_word.analyses:\n",
    "            analysis_dict = disambig_word.analyses[0][1]\n",
    "            if 'd3tok' in analysis_dict:\n",
    "                d3tok = dediac_ar(analysis_dict['d3tok']).replace(\"_+\", \" +\").replace(\"+_\", \"+ \")\n",
    "                d3tok_forms.append(d3tok)\n",
    "            else:\n",
    "                d3tok_forms.append(disambig_word.word)\n",
    "        else:\n",
    "            d3tok_forms.append(disambig_word.word)\n",
    "    return \" \".join(d3tok_forms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3473c-7e72-4cc7-a264-ae601aaea8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c0ee1-ef42-43e3-bc05-dae799534faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6245ab6-5f7c-4c70-a2d3-3083379aaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 3. DATA LOADING, MAPPING, AND COMBINING\n",
    "# =====================================================================================\n",
    "def load_or_preprocess_data(disambiguator):\n",
    "    \"\"\"\n",
    "    Loads preprocessed combined data if it exists. Otherwise, it loads BAREC and Osman\n",
    "    data, standardizes, maps, combines, and preprocesses them with D3Tok format.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and Preparing Combined Readability Data ---\")\n",
    "    if os.path.exists(TRAIN_PREPROCESSED_PATH) and os.path.exists(DEV_PREPROCESSED_PATH):\n",
    "        print(f\"✔ Found preprocessed combined files. Loading them from:\\n- {TRAIN_PREPROCESSED_PATH}\\n- {DEV_PREPROCESSED_PATH}\")\n",
    "        train_df = pd.read_csv(TRAIN_PREPROCESSED_PATH)\n",
    "        val_df = pd.read_csv(DEV_PREPROCESSED_PATH)\n",
    "        train_df['text'] = train_df['text'].astype(str)\n",
    "        val_df['text'] = val_df['text'].astype(str)\n",
    "        print(f\"✔ Successfully loaded {len(train_df)} training and {len(val_df)} validation records.\")\n",
    "        return train_df, val_df\n",
    "    else:\n",
    "        print(\"Preprocessed combined files not found. Starting one-time data integration and preprocessing...\")\n",
    "        try:\n",
    "            # --- Load BAREC Data (assumed to be comma-separated) ---\n",
    "            print(\"\\n1. Loading BAREC data...\")\n",
    "            barec_train_df = pd.read_csv(BAREC_TRAIN_PATH)\n",
    "            barec_val_df = pd.read_csv(BAREC_DEV_PATH)\n",
    "            barec_train_df.rename(columns={'Sentence': 'text', 'Readability_Level_19': 'label'}, inplace=True)\n",
    "            barec_val_df.rename(columns={'Sentence': 'text', 'Readability_Level_19': 'label'}, inplace=True)\n",
    "            print(f\"  - Loaded {len(barec_train_df)} BAREC training records.\")\n",
    "            print(f\"  - Loaded {len(barec_val_df)} BAREC validation records.\")\n",
    "\n",
    "            # --- Load and Map Osman Data ---\n",
    "            print(\"\\n2. Loading and mapping Osman data...\")\n",
    "            \n",
    "            # ===== FIX IS HERE =====\n",
    "            # The Osman files are separated by Tabs (\\t), not commas. We must specify this.\n",
    "            osman_train_df = pd.read_csv(OSMAN_TRAIN_PATH, sep='\\t')\n",
    "            osman_dev_df = pd.read_csv(OSMAN_DEV_PATH, sep='\\t')\n",
    "            # =======================\n",
    "\n",
    "            for df, name in [(osman_train_df, \"training\"), (osman_dev_df, \"validation\")]:\n",
    "                text_col = 'Text'\n",
    "                label_col = 'Fine-grained'\n",
    "\n",
    "                if text_col not in df.columns or label_col not in df.columns:\n",
    "                    raise ValueError(f\"Error in Osman {name} data: Columns '{text_col}' or '{label_col}' not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "                df.rename(columns={text_col: 'text', label_col: 'label'}, inplace=True)\n",
    "                df['label'] = df['label'].str.replace('G', '', regex=False).astype(int)\n",
    "                print(f\"  - Loaded and mapped {len(df)} Osman {name} records.\")\n",
    "\n",
    "            # --- Combine Datasets ---\n",
    "            print(\"\\n3. Combining BAREC and Osman datasets...\")\n",
    "            train_df = pd.concat([barec_train_df[['text', 'label']], osman_train_df[['text', 'label']]], ignore_index=True)\n",
    "            val_df = pd.concat([barec_val_df[['text', 'label']], osman_dev_df[['text', 'label']]], ignore_index=True)\n",
    "            print(f\"  - Combined training data size: {len(train_df)} records.\")\n",
    "            print(f\"  - Combined validation data size: {len(val_df)} records.\")\n",
    "\n",
    "            # --- Final Cleaning and Label Normalization ---\n",
    "            train_df.dropna(subset=['text', 'label'], inplace=True)\n",
    "            val_df.dropna(subset=['label', 'text'], inplace=True)\n",
    "            train_df['text'] = train_df['text'].astype(str)\n",
    "            val_df['text'] = val_df['text'].astype(str)\n",
    "            \n",
    "            train_df['label'] = train_df['label'].astype(int) - 1\n",
    "            val_df['label'] = val_df['label'].astype(int) - 1\n",
    "            train_df['label'] = train_df['label'].astype(float)\n",
    "            val_df['label'] = val_df['label'].astype(float)\n",
    "            \n",
    "            print(\"\\n4. Preprocessing all text to D3Tok format (this will only run once)...\")\n",
    "            train_df['text'] = train_df['text'].apply(lambda x: preprocess_d3tok(x, disambiguator))\n",
    "            val_df['text'] = val_df['text'].apply(lambda x: preprocess_d3tok(x, disambiguator))\n",
    "            print(\"✔ Text preprocessing finished.\")\n",
    "\n",
    "            print(\"\\n5. Saving combined preprocessed data for future use...\")\n",
    "            train_df.to_csv(TRAIN_PREPROCESSED_PATH, index=False)\n",
    "            val_df.to_csv(DEV_PREPROCESSED_PATH, index=False)\n",
    "            print(f\"** Saved preprocessed files to {TRAIN_PREPROCESSED_PATH} and {DEV_PREPROCESSED_PATH} **\")\n",
    "            \n",
    "            return train_df, val_df\n",
    "\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            print(f\"\\n! SCRIPT STOPPED DUE TO AN ERROR: {e}\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"! An unexpected error occurred during initial data processing: {e}\")\n",
    "            return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e500a4-2bd2-473d-aefb-b4694434fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT Disambiguator for preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Fatima\\AppData\\Roaming\\camel_tools\\data\\disambig_bert_unfactored\\msa were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Combined Readability Data ---\n",
      "✔ Found preprocessed combined files. Loading them from:\n",
      "- .\\data\\train_combined_preprocessed_d3tok.csv\n",
      "- .\\data\\dev_combined_preprocessed_d3tok.csv\n",
      "✔ Successfully loaded 64548 training and 8690 validation records.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 4. INITIALIZATION AND DATASET PREPARATION\n",
    "# =====================================================================================\n",
    "print(\"Initializing BERT Disambiguator for preprocessing...\")\n",
    "bert_disambiguator = BERTUnfactoredDisambiguator.pretrained('msa')\n",
    "\n",
    "train_df, val_df = load_or_preprocess_data(bert_disambiguator)\n",
    "\n",
    "if train_df is None:\n",
    "    print(\"Stopping script due to data loading failure.\")\n",
    "    exit()\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- PyTorch Dataset Class ---\n",
    "class ReadabilityDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.get('input_ids', []))\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.flatten()\n",
    "    rounded_preds = np.round(preds)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1).astype(int)\n",
    "    labels = p.label_ids.astype(int)\n",
    "    qwk = cohen_kappa_score(labels, clipped_preds, weights='quadratic')\n",
    "    return {\"qwk\": qwk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62838d9-4a82-4297-926f-b06203423b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INITIALIZING REGRESSION MODEL AND TRAINER =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\barec_env\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on combined dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40350' max='40350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40350/40350 2:26:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Qwk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.107400</td>\n",
       "      <td>3.829960</td>\n",
       "      <td>0.797497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.799800</td>\n",
       "      <td>3.850561</td>\n",
       "      <td>0.800449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.359100</td>\n",
       "      <td>3.910313</td>\n",
       "      <td>0.809473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.052800</td>\n",
       "      <td>3.846371</td>\n",
       "      <td>0.811091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>3.899869</td>\n",
       "      <td>0.808017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>3.748262</td>\n",
       "      <td>0.814835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>3.792102</td>\n",
       "      <td>0.812451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>3.693799</td>\n",
       "      <td>0.815007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>3.702644</td>\n",
       "      <td>0.817543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>3.691167</td>\n",
       "      <td>0.818368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Training finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 5. MODEL TRAINING\n",
    "# =====================================================================================\n",
    "print(\"\\n===== INITIALIZING REGRESSION MODEL AND TRAINER =====\\n\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "train_dataset = ReadabilityDataset(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "val_dataset = ReadabilityDataset(val_df['text'].tolist(), val_df['label'].tolist())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-5, \n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "print(\"Starting training on combined dataset...\")\n",
    "trainer.train()\n",
    "print(\"✔ Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4dd075-94bd-4c32-8b71-9ba4bf8b34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INITIALIZING REGRESSION MODEL AND TRAINER =====\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =====================================================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 5. MODEL TRAINING\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =====================================================================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== INITIALIZING REGRESSION MODEL AND TRAINER =====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, num_labels\u001b[38;5;241m=\u001b[39mNUM_LABELS)\n\u001b[0;32m      6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ReadabilityDataset(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      7\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ReadabilityDataset(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoModelForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 5. MODEL TRAINING\n",
    "# =====================================================================================\n",
    "print(\"\\n===== INITIALIZING REGRESSION MODEL AND TRAINER =====\\n\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "train_dataset = ReadabilityDataset(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "val_dataset = ReadabilityDataset(val_df['text'].tolist(), val_df['label'].tolist())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "\n",
    "# --- MODIFICATION FOR RESUMING ---\n",
    "# Check if a checkpoint exists in the output directory\n",
    "if os.path.isdir(CHECKPOINT_DIR):\n",
    "    # Find the latest checkpoint directory (e.g., 'checkpoint-4044')\n",
    "    checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-48420\")]\n",
    "    if checkpoints:\n",
    "        # Sort by step number to get the latest one\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "        print(f\"Resuming training from checkpoint: {latest_checkpoint_path}\")\n",
    "        trainer.train(resume_from_checkpoint=latest_checkpoint_path)\n",
    "    else:\n",
    "        # No checkpoints found, start training from scratch\n",
    "        print(\"No checkpoint found. Starting training from the beginning...\")\n",
    "        trainer.train()\n",
    "else:\n",
    "    # Output directory doesn't even exist, start fresh\n",
    "    print(\"No checkpoint directory found. Starting training from the beginning...\")\n",
    "    trainer.train()\n",
    "\n",
    "print(\"✔ Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f62e9d-d51e-4be9-827e-cf97c6235c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INITIALIZING REGRESSION MODEL AND TRAINER =====\n",
      "\n",
      "Resuming training from checkpoint: .\\results_dares_v2\\regression_combined_readability-arabertv2-d3tok-reg\\checkpoint-72630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\barec_env\\lib\\site-packages\\transformers\\trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72630' max='72630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72630/72630 : < :, Epoch 18/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Training finished.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 5. MODEL TRAINING\n",
    "# =====================================================================================\n",
    "print(\"\\n===== INITIALIZING REGRESSION MODEL AND TRAINER =====\\n\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "train_dataset = ReadabilityDataset(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "val_dataset = ReadabilityDataset(val_df['text'].tolist(), val_df['label'].tolist())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "\n",
    "# --- MODIFICATION FOR RESUMING ---\n",
    "# Check if a checkpoint exists in the output directory\n",
    "if os.path.isdir(CHECKPOINT_DIR):\n",
    "    # Find the latest checkpoint directory (e.g., 'checkpoint-4044')\n",
    "    checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-72630\")]\n",
    "    if checkpoints:\n",
    "        # Sort by step number to get the latest one\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "        print(f\"Resuming training from checkpoint: {latest_checkpoint_path}\")\n",
    "        trainer.train(resume_from_checkpoint=latest_checkpoint_path)\n",
    "    else:\n",
    "        # No checkpoints found, start training from scratch\n",
    "        print(\"No checkpoint found. Starting training from the beginning...\")\n",
    "        trainer.train()\n",
    "else:\n",
    "    # Output directory doesn't even exist, start fresh\n",
    "    print(\"No checkpoint directory found. Starting training from the beginning...\")\n",
    "    trainer.train()\n",
    "\n",
    "print(\"✔ Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cfcf477-945a-4712-88ab-9f672e64a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL PREDICTION AND SUBMISSION =====\n",
      "\n",
      "Preprocessing blind test text to D3Tok format...\n",
      "Generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prediction file to: .\\submission\\submission_regression_combined_final.csv\n",
      "\n",
      "Compressing submission_regression_combined_final.csv into submission_regression_combined_final.zip...\n",
      "✔ Submission file submission_regression_combined_final.zip created successfully.\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 6. FINAL PREDICTION AND SUBMISSION\n",
    "# =====================================================================================\n",
    "print(\"\\n===== FINAL PREDICTION AND SUBMISSION =====\\n\")\n",
    "try:\n",
    "    test_df = pd.read_csv(BLIND_TEST_PATH)\n",
    "    test_df.dropna(subset=['Sentence'], inplace=True)\n",
    "\n",
    "    print(\"Preprocessing blind test text to D3Tok format...\")\n",
    "    # FIX 1: Use the correct variable name 'bert_disambiguator'\n",
    "    test_df['processed_text'] = test_df['Sentence'].apply(lambda x: preprocess_d3tok(x, bert_disambiguator))\n",
    "\n",
    "    print(\"Generating predictions on the test set...\")\n",
    "    test_dataset = ReadabilityDataset(test_df['processed_text'].tolist())\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    raw_preds = predictions.predictions.flatten()\n",
    "    rounded_preds = np.round(raw_preds)\n",
    "    clipped_preds = np.clip(rounded_preds, 0, TARGET_CLASSES - 1)\n",
    "\n",
    "    test_df['Prediction'] = (clipped_preds + 1).astype(int)\n",
    "\n",
    "    # FIX 2: Use the 'ID' column from the test file and rename it to 'Sentence ID' for the submission.\n",
    "    submission_df = test_df[['ID', 'Prediction']]\n",
    "    submission_df = submission_df.rename(columns={'ID': 'Sentence ID'})\n",
    "    \n",
    "    print(f\"Saving prediction file to: {SUBMISSION_PATH}\")\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "    print(f\"\\nCompressing {os.path.basename(SUBMISSION_PATH)} into {os.path.basename(ZIPPED_SUBMISSION_PATH)}...\")\n",
    "    with zipfile.ZipFile(ZIPPED_SUBMISSION_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(SUBMISSION_PATH, arcname=os.path.basename(SUBMISSION_PATH))\n",
    "\n",
    "    print(f\"✔ Submission file {os.path.basename(ZIPPED_SUBMISSION_PATH)} created successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"! ERROR: Test file not found. Make sure 'blind_test_data.csv' is in the '{DATA_DIR}' directory.\")\n",
    "except KeyError:\n",
    "    # Add a more specific error message for this common problem\n",
    "    print(\"! KEY ERROR: Could not find the expected 'ID' column in 'blind_test_data.csv'. Please check the file's column names.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809428f-1b47-46cc-8efc-be53d52a25ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (barec_env)",
   "language": "python",
   "name": "barec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
